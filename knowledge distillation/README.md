


- One-for-All: Bridge the Gap Between Heterogeneous Architectures in Knowledge Distillation. (NIPS, 2023) [[paper](http://arxiv.org/abs/2310.19444)] (emmm; empirical)
- Graph-less Collaborative Filtering. (WWW, 2023) [[paper](http://arxiv.org/abs/2303.08537)]
- Distillation from Heterogeneous Models for Top-K Recommendation. (WWW, 2023) [[paper](http://arxiv.org/abs/2303.01130)]
- Unbiased Knowledge Distillation for Recommendation. (WSDM, 2023) [[paper](http://arxiv.org/abs/2211.14729)]
- Decoupled Knowledge Distillation. (CVPR, 2022) [[paper](http://arxiv.org/abs/2203.08679)] (novel; theoretical)
- Improved Feature Distillation via Projector Ensemble. (NIPS, 2022) [[paper](http://arxiv.org/abs/2210.15274)]
- Knowledge Distillation from A Stronger Teacher. (NIPS, 2022) [[paper](http://arxiv.org/abs/2205.10536)]
- Distilling Knowledge from Graph Convolutional Networks. (CVPR, 2021) [[paper](http://arxiv.org/abs/2003.10477)] (emmm; empirical; graph)
- Distilling Knowledge via Knowledge Review. (CVPR, 2021) [[paper](http://arxiv.org/abs/2104.09044)] (emmm; empirical)
- Topology Distillation for Recommender System. (KDD, 2021) [[paper](http://arxiv.org/abs/2106.08700)]
- Bidirectional Distillation for Top-K Recommender System. (WWW, 2021) [[paper](http://arxiv.org/abs/2106.02870)]
- DE-RRD: A Knowledge Distillation Framework for Recommender System. (CIKM, 2020) [[paper](http://arxiv.org/abs/2012.04357)]
- Collaborative Distillation for Top-N Recommendation. (ICDM, 2019) [[paper](http://arxiv.org/abs/1911.05276)]
- Relational Knowledge Distillation. (CVPR, 2018) [[paper](http://arxiv.org/abs/1904.05068)]
- Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System. (KDD, 2018) [[paper](http://arxiv.org/abs/1809.07428)]
- FitNets: Hints for Thin Deep Nets. (ICLR, 2015) [[paper](http://arxiv.org/abs/1412.6550)]