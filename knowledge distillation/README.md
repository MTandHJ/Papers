


- Graph-less Collaborative Filtering. (WWW, 2023) [[paper](http://arxiv.org/abs/2303.08537)]
- Distillation from Heterogeneous Models for Top-K Recommendation. (WWW, 2023) [[paper](http://arxiv.org/abs/2303.01130)]
- Unbiased Knowledge Distillation for Recommendation. (WSDM, 2023) [[paper](http://arxiv.org/abs/2211.14729)]
- Improved Feature Distillation via Projector Ensemble. (NIPS, 2022) [[paper](http://arxiv.org/abs/2210.15274)]
- Knowledge Distillation from A Stronger Teacher. (NIPS, 2022) [[paper](http://arxiv.org/abs/2205.10536)]
- Topology Distillation for Recommender System. (KDD, 2021) [[paper](http://arxiv.org/abs/2106.08700)]
- Bidirectional Distillation for Top-K Recommender System. (WWW, 2021) [[paper](http://arxiv.org/abs/2106.02870)]
- DE-RRD: A Knowledge Distillation Framework for Recommender System. (CIKM, 2020) [[paper](http://arxiv.org/abs/2012.04357)]
- Collaborative Distillation for Top-N Recommendation. (ICDM, 2019) [[paper](http://arxiv.org/abs/1911.05276)]
- Relational Knowledge Distillation. (CVPR, 2018) [[paper](http://arxiv.org/abs/1904.05068)]
- Ranking Distillation: Learning Compact Ranking Models With High Performance for Recommender System. (KDD, 2018) [[paper](http://arxiv.org/abs/1809.07428)]